# ============================================================================
# EventGlucose Environment Configuration Template
# ============================================================================
# Usage:
#   1. Copy this file: cp env.sh.template env.sh
#   2. Fill in your API keys and credentials
#   3. Source it: source env.sh
#
# SECURITY WARNING: Never commit env.sh with real credentials!
# The env.sh file is gitignored to prevent accidental commits.
# ============================================================================

# ============================================================================
# rclone Configuration (Google Drive Shared Drive) - OPTIONAL
# ============================================================================
# Only needed if syncing data with Google Drive
# Get token: rclone authorize "drive"

# export RCLONE_CONFIG_GDRIVE_TYPE=drive
# export RCLONE_CONFIG_GDRIVE_SCOPE=drive
# export RCLONE_CONFIG_GDRIVE_TOKEN='YOUR_RCLONE_TOKEN_HERE'
# export RCLONE_CONFIG_GDRIVE_TEAM_DRIVE="YOUR_TEAM_DRIVE_ID"

# ============================================================================
# Remote Storage Root (Google Drive replaces S3) - OPTIONAL
# ============================================================================
# export REMOTE_ROOT='gdrive:_EventGlucose'


# ============================================================================
# Local Workspace Paths (on this machine)
# ============================================================================
# Three-folder structure: Data, Model, Results
export LOCAL_WORKSPACE='_WorkSpace'
export LOCAL_ROOT='_WorkSpace'

# Data folder - All datasets and input data
export LOCAL_DATA_FOLDER='_WorkSpace/Data'
export LOCAL_SOURCE_STORE='_WorkSpace/Data/1-SourceStore'            # Raw data
export LOCAL_RECORD_STORE='_WorkSpace/Data/2-RecStore'               # Processed patient records
export LOCAL_CASE_STORE='_WorkSpace/Data/3-CaseStore'                # Event-triggered cases
export LOCAL_AIDATA_STORE='_WorkSpace/Data/4-AIDataStore'            # ML-ready datasets
export LOCAL_EXTERNAL_STORE='_WorkSpace/Data/ExternalStore'          # Reference data
export LOCAL_REFERENCE_STORE='_WorkSpace/Data/ExternalStore/@inference'

# Model folder - All model weights and trained models
export LOCAL_MODEL_FOLDER='_WorkSpace/Model'
export LOCAL_MODEL_STORE='_WorkSpace/Model'                          # Model weights storage
export LOCAL_MODELINSTANCE_STORE='_WorkSpace/Model'                  # Trained models
export LOCAL_ENDPOINT_STORE='_WorkSpace/Model/Endpoints'             # Deployment packages

# Hugging Face cache configuration
export HF_HOME="${LOCAL_MODEL_STORE}/huggingface_cache"              # Hugging Face cache location

# Results folder - All inference outputs, caches, and metrics
export LOCAL_RESULTS_FOLDER='_WorkSpace/Result'
export LOCAL_AGENTWORKSPACE_STORE='_WorkSpace/Result/AgentWorkspace'  # Agent workspace

export EXTERNAL_VERSION="@v1215"                                # External data version

# ============================================================================
# Remote Workspace Paths (Google Drive Shared Drive) - OPTIONAL
# ============================================================================
# Only needed if syncing with remote storage
# export REMOTE_RAWDATA_STORE="${REMOTE_ROOT}/0-RawDataStore/"
# export REMOTE_SOURCE_STORE="${REMOTE_ROOT}/1-SourceStore/"
# export REMOTE_RECORD_STORE="${REMOTE_ROOT}/2-RecStore/"
# export REMOTE_CASE_STORE="${REMOTE_ROOT}/3-CaseStore/"
# export REMOTE_AIDATA_STORE="${REMOTE_ROOT}/4-AIDataStore/"
# export REMOTE_MODELINSTANCE_STORE="${REMOTE_ROOT}/5-ModelInstanceStore/"
# export REMOTE_ENDPOINT_STORE="${REMOTE_ROOT}/6-EndpointStore/"
# export REMOTE_EXTERNAL_STORE="${REMOTE_ROOT}/ExternalStore/"


############################################################################
# API Keys Configuration
############################################################################
# IMPORTANT: Replace placeholder values with your actual API keys
# Never commit this file with real credentials!

# OpenAI API - Required for GPT models and DirectPrompt baseline
# Get key: https://platform.openai.com/api-keys
export OPENAI_API_KEY='your-openai-api-key-here'

# OpenRouter API - Required for accessing various models via OpenRouter
# Get key: https://openrouter.ai/keys
export OPENROUTER_API_KEY='your-openrouter-api-key-here'

# LangSmith API - Optional, for tracing and monitoring
# Get key: https://smith.langchain.com/
export LANGSMITH_API_KEY='your-langsmith-api-key-here'

# Hugging Face Token - Required for downloading gated models
# Get token: https://huggingface.co/settings/tokens
export HF_TOKEN='your-huggingface-token-here'

# SSL Certificate Configuration (for Zscaler/corporate proxy)
# Uncomment and update paths if behind corporate proxy
# export SSL_CERT_FILE="/path/to/cacert.pem"
# export REQUESTS_CA_BUNDLE="/path/to/cacert.pem"
# export CURL_CA_BUNDLE="/path/to/cacert.pem"


# ============================================================================
# EventGlucose/CIK Configuration (for config.py compatibility)
# ============================================================================

# DATA FOLDER: All datasets and input files go here
export CIK_DATA_STORE="${LOCAL_DATA_FOLDER}"                         # Main data storage → _WorkSpace/Data
export CIK_DOMINICK_STORE="${LOCAL_DATA_FOLDER}/dominicks"           # Dominick's dataset
export CIK_TRAFFIC_DATA_STORE="${LOCAL_DATA_FOLDER}/traffic_data"    # Traffic dataset

# MODEL FOLDER: All model weights and checkpoints go here
export CIK_MODEL_STORE="${LOCAL_MODEL_FOLDER}"                       # Model weights → _WorkSpace/Model

# RESULTS FOLDER: All inference outputs, caches, and metrics go here
export CIK_RESULT_CACHE="${LOCAL_RESULTS_FOLDER}/_InferenceCache"              # Prediction cache
export CIK_METRIC_SCALING_CACHE="${LOCAL_RESULTS_FOLDER}/_MetricScalingCache"  # Metric scaling factors
export CIK_METRIC_COMPUTE_VARIANCE="true"



# OpenAI configuration (CIK_ prefixed for config.py)
export CIK_OPENAI_API_KEY="${OPENAI_API_KEY}"  # Use the same OpenAI key
export CIK_OPENAI_USE_AZURE="False"            # Use OpenAI directly (not Azure)
export CIK_OPENAI_API_VERSION=""               # Leave empty for OpenAI (only needed for Azure)
export CIK_OPENAI_AZURE_ENDPOINT=""            # Leave empty for OpenAI (only needed for Azure)

# Llama-405b configuration (if using vLLM server) - OPTIONAL
export CIK_LLAMA31_405B_URL=""      # Set to your vLLM server URL if available
export CIK_LLAMA31_405B_API_KEY=""  # Set to your Llama API key if available

# Nixtla TimeGEN configuration - Required for TimeGEN baseline
# Get key: https://nixtlaverse.nixtla.io/
export CIK_NIXTLA_BASE_URL="None"  # Azure API URL for Nixtla TimeGEN (or leave as "None")
export CIK_NIXTLA_API_KEY="your-nixtla-api-key-here"


# Get absolute path to .venv (works even if sourced from different directory)
export PYTHON_VENV="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/.venv"
