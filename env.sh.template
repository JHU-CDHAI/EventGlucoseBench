# ============================================================================
# EventGlucose Environment Configuration Template
# ============================================================================
# Usage:
#   1. Copy this file: cp env.sh.template env.sh
#   2. Fill in your API keys and credentials
#   3. Source it: source env.sh

# ============================================================================
# Local Workspace Paths (on this machine)
# ============================================================================
# Three-folder structure: Data, Model, Results
export LOCAL_WORKSPACE='_WorkSpace'
export LOCAL_ROOT='_WorkSpace'

# Data folder - All datasets and input data
export LOCAL_DATA_FOLDER='_WorkSpace/Data'
export LOCAL_SOURCE_STORE='_WorkSpace/Data/1-SourceStore'            # Raw data
export LOCAL_RECORD_STORE='_WorkSpace/Data/2-RecStore'               # Processed patient records
export LOCAL_CASE_STORE='_WorkSpace/Data/3-CaseStore'                # Event-triggered cases


# Model folder - All model weights and trained models
export LOCAL_MODEL_FOLDER='_WorkSpace/Model'
export LOCAL_MODEL_STORE='_WorkSpace/Model'                          # Model weights storage
export LOCAL_MODELINSTANCE_STORE='_WorkSpace/Model'                  # Trained models
export LOCAL_ENDPOINT_STORE='_WorkSpace/Model/Endpoints'             # Deployment packages

# Hugging Face cache configuration
export HF_HOME="${LOCAL_MODEL_STORE}/huggingface_cache"              # Hugging Face cache location

# Results folder - All inference outputs, caches, and metrics
export LOCAL_RESULTS_FOLDER='_WorkSpace/Result'





############################################################################
# API Keys Configuration
############################################################################

# OpenAI API - Required for GPT models and DirectPrompt baseline
# Get key: https://platform.openai.com/api-keys
export OPENAI_API_KEY='your-openai-api-key-here'

# OpenRouter API - Required for accessing various models via OpenRouter
# Get key: https://openrouter.ai/keys
export OPENROUTER_API_KEY='your-openrouter-api-key-here'

# LangSmith API - Optional, for tracing and monitoring
# Get key: https://smith.langchain.com/
export LANGSMITH_API_KEY='your-langsmith-api-key-here'

# Hugging Face Token - Required for downloading gated models
# Get token: https://huggingface.co/settings/tokens
export HF_TOKEN='your-huggingface-token-here'

# SSL Certificate Configuration (for Zscaler/corporate proxy)
# Uncomment and update paths if behind corporate proxy
# export SSL_CERT_FILE="/path/to/cacert.pem"
# export REQUESTS_CA_BUNDLE="/path/to/cacert.pem"
# export CURL_CA_BUNDLE="/path/to/cacert.pem"


# ============================================================================
# EventGlucose/CIK Configuration (for config.py compatibility)
# ============================================================================

# DATA FOLDER: All datasets and input files go here
export CIK_DATA_STORE="${LOCAL_DATA_FOLDER}"                         # Main data storage → _WorkSpace/Data
export CIK_DOMINICK_STORE="${LOCAL_DATA_FOLDER}/dominicks"           # Dominick's dataset
export CIK_TRAFFIC_DATA_STORE="${LOCAL_DATA_FOLDER}/traffic_data"    # Traffic dataset

# MODEL FOLDER: All model weights and checkpoints go here
export CIK_MODEL_STORE="${LOCAL_MODEL_FOLDER}"                       # Model weights → _WorkSpace/Model

# RESULTS FOLDER: All inference outputs, caches, and metrics go here
export CIK_RESULT_CACHE="${LOCAL_RESULTS_FOLDER}/_InferenceCache"              # Prediction cache
export CIK_METRIC_SCALING_CACHE="${LOCAL_RESULTS_FOLDER}/_MetricScalingCache"  # Metric scaling factors
export CIK_METRIC_COMPUTE_VARIANCE="true"



# OpenAI configuration (CIK_ prefixed for config.py)
export CIK_OPENAI_API_KEY="${OPENAI_API_KEY}"  # Use the same OpenAI key
export CIK_OPENAI_USE_AZURE="False"            # Use OpenAI directly (not Azure)
export CIK_OPENAI_API_VERSION=""               # Leave empty for OpenAI (only needed for Azure)
export CIK_OPENAI_AZURE_ENDPOINT=""            # Leave empty for OpenAI (only needed for Azure)

# Llama-405b configuration (if using vLLM server) - OPTIONAL
export CIK_LLAMA31_405B_URL=""      # Set to your vLLM server URL if available
export CIK_LLAMA31_405B_API_KEY=""  # Set to your Llama API key if available

# Nixtla TimeGEN configuration - Required for TimeGEN baseline
# Get key: https://nixtlaverse.nixtla.io/
export CIK_NIXTLA_BASE_URL="None"  # Azure API URL for Nixtla TimeGEN (or leave as "None")
export CIK_NIXTLA_API_KEY="your-nixtla-api-key-here"


# Get absolute path to .venv (works even if sourced from different directory)
export PYTHON_VENV="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)/.venv"
